---
title: Postgres
date: 2025-03-12
type: book
sidebar_position: 1
---

import Link from '/src/components/Link';
import Figure from '/src/components/Figure';
import BZhan from '/src/components/BZhan';
import PluginFields from '/src/components/PluginFields';

## 数据源配置

<Figure img={require('./datasource.png')}/>

* **配置项说明:** 

<PluginFields>

1. 实例ID

	* **类型:** 单行文本
	* **必须:** 是
	* **默认值:** 无
	* **说明:** 		数据源实例名称，请起一个有意义且唯一的名称

2. host

	* **类型:** 单行文本
	* **必须:** 是
	* **默认值:** 无
	* **说明:** 		服务器节点连接地址，可以为IP或者域名

3. 端口

	* **类型:** 整型数字
	* **必须:** 是
	* **默认值:** 5432
	* **说明:** 		无

4. 数据库名

	* **类型:** 单行文本
	* **必须:** 是
	* **默认值:** 无
	* **说明:** 		数据库名,创建JDBC实例时用

5. schema

	* **类型:** 单行文本
	* **必须:** 是
	* **默认值:** public
	* **说明:** 		Specify the schema (or several schema separated by commas) to be set in the search-path. This schema will be used to resolve unqualified object names used in statements over this connection.

6. 用户名

	* **类型:** 单行文本
	* **必须:** 是
	* **默认值:** root
	* **说明:** 		无

7. 密码

	* **类型:** 密码
	* **必须:** 是
	* **默认值:** 无
	* **说明:** 		无

8. 编码

	* **类型:** 单选
	* **必须:** 是
	* **默认值:** 无
	* **说明:** 		数据数据

9. 附加参数

	* **类型:** 单行文本
	* **必须:** 否
	* **默认值:** 无
	* **说明:** 		无


</PluginFields>

## 批量读

<Figure img={require('./dataxReader.png')}/>

* **配置项说明:** 

<PluginFields>

1. 数据库名

	* **类型:** 单选
	* **必须:** 是
	* **默认值:** 无
	* **说明:** 		无

2. splitPk

	* **类型:** 单选
	* **必须:** 是
	* **默认值:** false
	* **说明:** 		描述：数据抽取时，如果指定splitPk，表示用户希望使用splitPk代表的字段进行数据分片，DataX因此会启动并发任务进行数据同步，这样可以大大提供数据同步的效能。推荐splitPk用户使用表主键，因为表主键通常情况下比较均匀，因此切分出来的分片也不容易出现数据热点。
		目前splitPk仅支持整形数据切分，不支持浮点、字符串型、日期等其他类型。如果用户指定其他非支持类型，将报错！
		splitPk设置为空，底层将视作用户不允许对单表进行切分，因此使用单通道进行抽取。

3. fetchSize

	* **类型:** 整型数字
	* **必须:** 是
	* **默认值:** 2000
	* **说明:** 		描述：该配置项定义了插件和数据库服务器端每次批量数据获取条数，该值决定了DataX和服务器端的网络交互次数，能够较大的提升数据抽取性能。注意，该值过大(>2048)可能造成DataX进程OOM

4. 配置模版

	* **类型:** 富文本
	* **必须:** 是
	* **默认值:** com.qlangtech.tis.plugin.datax.DataXPostgresqlReader.getDftTemplate()
	* **说明:** 		无特殊情况请不要修改模版内容，避免不必要的错误


</PluginFields>

## 批量写

<Figure img={require('./dataxWriter.png')}/>

* **配置项说明:** 

<PluginFields>

1. 数据库名

	* **类型:** 单选
	* **必须:** 是
	* **默认值:** 无
	* **说明:** 		无

2. preSql

	* **类型:** 富文本
	* **必须:** 否
	* **默认值:** 无
	* **说明:** 

		描述：写入数据到目的表前，会先执行这里的标准语句。如果 Sql 中有你需要操作到的表名称，请使用 `@table` 表示，这样在实际执行 Sql 语句时，会对变量按照实际表名称进行替换。比如你的任务是要写入到目的端的100个同构分表(表名称为:datax_00,datax01, ... datax_98,datax_99)，并且你希望导入数据前，先对表中数据进行删除操作，那么你可以这样配置：`"preSql":["delete from 表名"]`，效果是：在执行到每个表写入数据前，会先执行对应的 delete from 对应表名称

3. postSql

	* **类型:** 富文本
	* **必须:** 否
	* **默认值:** 无
	* **说明:** 

		写入数据到目的表后，会执行这里的标准语句。（原理同 preSql ）

4. 自动建表

	* **类型:** 单选
	* **必须:** 是
	* **默认值:** on
	* **说明:** 

		解析Reader的元数据，自动生成Writer create table DDL语句，有三种选择：
		* `off`：关闭自动生成及同步目标端建表DDL语句，当目标端表实例已经存在可选择此选项。
		* `default`：打开动生成及自动执行目标端建表DDL语句，执行任务状态由程序自动控制毋需人为干涉。
		* `customized`：用户可自定义设置`自动执行目标端建表DDL语句逻辑`，如：是否需要生成列注释等。

5. batchSize

	* **类型:** 整型数字
	* **必须:** 否
	* **默认值:** 1000
	* **说明:** 

		* 描述：一次性批量提交的记录数大小，该值可以极大减少DataX与Mysql的网络交互次数，并提升整体吞吐量。但是该值设置过大可能会造成DataX运行进程OOM情况。

6. 配置模版

	* **类型:** 富文本
	* **必须:** 是
	* **默认值:** com.qlangtech.tis.plugin.datax.DataXPostgresqlWriter.getDftTemplate()
	* **说明:** 		无特殊情况请不要修改模版内容，避免不必要的错误


</PluginFields>

## 实时读

<Figure img={require('./mq.png')}/>

* **配置项说明:** 

<PluginFields>

1. 解码器

	* **类型:** 单选
	* **必须:** 是
	* **默认值:** decoderbufs
	* **说明:** 		The name of the Postgres logical decoding plug-in installed on the server. Supported values are decoderbufs, wal2json, wal2json_rds, wal2json_streaming, wal2json_rds_streaming and pgoutput.

2. 起始位点

	* **类型:** 单选
	* **必须:** 是
	* **默认值:** latest
	* **说明:** 

		Debezium startup options
		
		参数详细请参考：[https://ververica.github.io/flink-cdc-connectors/master/content/connectors/postgres-cdc.html#incremental-snapshot-options](https://ververica.github.io/flink-cdc-connectors/master/content/connectors/postgres-cdc.html#incremental-snapshot-options)
		
		* `Initial`:
		  Performs an initial snapshot on the monitored database tables upon first startup, and continue to read the latest binlog.     
		
		* `Latest`:
		  Never to perform snapshot on the monitored database tables upon first startup, just read from the end of the binlog which means only have the changes since the connector was started.

3. REPLICA IDENTITY

	* **类型:** 单选
	* **必须:** 是
	* **默认值:** DEFAULT
	* **说明:** 

		在 PostgreSQL 中，ALTER TABLE ... REPLICA IDENTITY 命令用于指定在逻辑复制或行级触发器中如何标识已更新或删除的行。https://developer.aliyun.com/ask/575334
		
		可选项有以下两个
		* `FULL`: 使用此值需要确保对应的表执行`ALTER TABLE your_table_name REPLICA IDENTITY FULL;`，表记录更新时会带上更新Before值，使用此方式比较耗费性能。
		* `DEFAULT`: 默认值，更新删除操作时不会带上Before值。


</PluginFields>

## 实时写

<Figure img={require('./sinkFactory.png')}/>

* **配置项说明:** 

<PluginFields>

1. semantic

	* **类型:** 单选
	* **必须:** 是
	* **默认值:** at-least-once
	* **说明:** 

		**描述：** sink 端是否支持二阶段提交
		
		**注意：**
		    如果此参数为空，默认不开启二阶段提交，即 sink 端不支持 exactly_once 语义；
		    当前只支持 exactly-once 和 at-least-once

2. 脚本类型

	* **类型:** 单行文本
	* **必须:** 是
	* **默认值:** StreamAPI
	* **说明:** 

		TIS 为您自动生成 Flink Stream 脚本，现支持两种类型脚本：
		
		* `SQL`: **优点**逻辑清晰，便于用户自行修改执行逻辑
		* `Stream API`：**优点**基于系统更底层执行逻辑执行、轻量、高性能

3. batchSize

	* **类型:** 整型数字
	* **必须:** 是
	* **默认值:** 5000
	* **说明:** 

		描述：一次性批量提交的记录数大小，该值可以极大减少 ChunJun 与数据库的网络交互次数，并提升整体吞吐量。但是该值设置过大可能会造成 ChunJun 运行进程 OOM 情况

4. flushIntervalMills

	* **类型:** 整型数字
	* **必须:** 是
	* **默认值:** 10000
	* **说明:** 		"the flush interval mills, over this time, asynchronous threads will flush data. The default value is 10s.

5. parallelism

	* **类型:** 整型数字
	* **必须:** 是
	* **默认值:** 1
	* **说明:** 		sink 并行度


</PluginFields>



